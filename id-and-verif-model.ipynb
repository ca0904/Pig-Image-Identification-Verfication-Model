{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0e000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import zipfile\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import cv2\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from optuna.pruners import MedianPruner\n",
    "from optuna.samplers import TPESampler\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    precision_recall_fscore_support,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.optim import SGD, AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, StepLR\n",
    "from torch.utils.data import DataLoader, Dataset, WeightedRandomSampler\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13bbcb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "SINGLE_COL_WIDTH = 6.5\n",
    "SINGLE_COL_HEIGHT = 4.0\n",
    "SQUARE_SIZE = 4.0\n",
    "IMAGE_DIR = \"pig-images/\"\n",
    "SAVE_DIR = \"weights/\"\n",
    "\n",
    "mpl.rcParams.update(\n",
    "    {\n",
    "        \"savefig.bbox\": \"tight\",\n",
    "        \"savefig.format\": \"png\",\n",
    "        \"font.family\": \"serif\",\n",
    "        \"font.size\": 10,\n",
    "        \"axes.titlesize\": 12,\n",
    "        \"axes.labelsize\": 11,\n",
    "        \"xtick.labelsize\": 9,\n",
    "        \"ytick.labelsize\": 9,\n",
    "        \"legend.fontsize\": 9,\n",
    "        \"lines.linewidth\": 1.5,\n",
    "        \"axes.grid\": True,\n",
    "        \"grid.linestyle\": \":\",\n",
    "        \"grid.linewidth\": 0.5,\n",
    "    }\n",
    ")\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250b9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_fig(fig, filename):\n",
    "    fig.tight_layout()\n",
    "    fullpath = os.path.join(SAVE_DIR, filename)\n",
    "    fig.savefig(fullpath, dpi=600)\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "def save_hist(\n",
    "    data,\n",
    "    filename,\n",
    "    title,\n",
    "    xlabel,\n",
    "    figsize=(SINGLE_COL_WIDTH, SINGLE_COL_HEIGHT),\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.hist(data, bins=50, alpha=0.7)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    _save_fig(fig, filename)\n",
    "\n",
    "\n",
    "def save_dual_hist(\n",
    "    data1,\n",
    "    data2,\n",
    "    filename,\n",
    "    title,\n",
    "    labels,\n",
    "    threshold=None,\n",
    "    figsize=(SINGLE_COL_WIDTH, SINGLE_COL_HEIGHT),\n",
    "):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    ax.hist(data1, bins=50, alpha=0.6, label=labels[0])\n",
    "    ax.hist(data2, bins=50, alpha=0.6, label=labels[1])\n",
    "    if threshold is not None:\n",
    "        ax.axvline(\n",
    "            threshold, color=\"k\", linestyle=\"--\", label=f\"Threshold={threshold:.3f}\"\n",
    "        )\n",
    "    ax.legend(loc=\"best\")\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Similarity Score\" if threshold is not None else \"Pixels\")\n",
    "    ax.set_ylabel(\"Frequency\")\n",
    "    _save_fig(fig, filename)\n",
    "\n",
    "\n",
    "def save_line_panel(history, filename, figsize=(SINGLE_COL_WIDTH, SINGLE_COL_HEIGHT)):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=figsize)\n",
    "    ax1.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    ax1.plot(history[\"val_loss\"], label=\"Val Loss\")\n",
    "    ax2.plot(history[\"train_acc\"], label=\"Train Acc\")\n",
    "    ax2.plot(history[\"val_acc\"], label=\"Val Acc\")\n",
    "    ax1.set_title(\"Loss History\")\n",
    "    ax2.set_title(\"Accuracy History\")\n",
    "    ax1.set_xlabel(\"Epoch\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax2.set_xlabel(\"Epoch\")\n",
    "    ax2.set_ylabel(\"Accuracy\")\n",
    "    ax1.legend(loc=\"best\")\n",
    "    ax2.legend(loc=\"best\")\n",
    "    _save_fig(fig, filename)\n",
    "\n",
    "\n",
    "def save_square(plot_func, filename, title, xlabel, ylabel, annotate=None):\n",
    "    fig, ax = plt.subplots(figsize=(SQUARE_SIZE, SQUARE_SIZE))\n",
    "    plot_func(ax)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    if annotate:\n",
    "        ax.text(\n",
    "            0.05,\n",
    "            0.95,\n",
    "            annotate,\n",
    "            transform=ax.transAxes,\n",
    "            va=\"top\",\n",
    "            bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\"),\n",
    "        )\n",
    "    _save_fig(fig, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7095737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_blur_threshold(frames):\n",
    "    lap_vars = [\n",
    "        cv2.Laplacian(cv2.cvtColor(f, cv2.COLOR_BGR2GRAY), cv2.CV_64F).var()\n",
    "        for f in frames\n",
    "    ]\n",
    "    blur_threshold = np.percentile(lap_vars, 10)\n",
    "    save_hist(lap_vars, \"blur_threshold.png\", \"Laplacian Variance\", \"Variance\")\n",
    "    return blur_threshold\n",
    "\n",
    "\n",
    "def tune_size_threshold(frames):\n",
    "    widths = [f.shape[1] for f in frames]\n",
    "    heights = [f.shape[0] for f in frames]\n",
    "    width_threshold = np.percentile(widths, 10)\n",
    "    height_threshold = np.percentile(heights, 10)\n",
    "    save_dual_hist(\n",
    "        widths,\n",
    "        heights,\n",
    "        \"width_height_threshold.png\",\n",
    "        \"Image Size Distribution\",\n",
    "        [\"widths\", \"heights\"],\n",
    "    )\n",
    "    return width_threshold, height_threshold\n",
    "\n",
    "\n",
    "def filter_dataset(image_dir):\n",
    "    images, dataset = [], []\n",
    "    for dir in sorted(os.listdir(image_dir)):\n",
    "        for img in sorted(os.listdir(os.path.join(image_dir, dir))):\n",
    "            images.append(cv2.imread(os.path.join(image_dir, dir, img)))\n",
    "            dataset.append(os.path.join(image_dir, dir, img))\n",
    "\n",
    "    blur = tune_blur_threshold(images)\n",
    "    width, height = tune_size_threshold(images)\n",
    "\n",
    "    filtered_dataset = defaultdict(list)\n",
    "    for image, path in zip(images, dataset):\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        if laplacian_var < blur:\n",
    "            continue\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        if h < height or w < width:\n",
    "            continue\n",
    "\n",
    "        aspect_ratio = w / h\n",
    "        if not (0.75 <= aspect_ratio <= 1.33):\n",
    "            continue\n",
    "\n",
    "        dir = os.path.dirname(path).split(\"/\")[-1]\n",
    "        filtered_dataset[dir].append(path)\n",
    "\n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab1b4e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_splits(image_dir, known_unknown_split=0.7):\n",
    "    np.random.seed(42)\n",
    "    pig_folders = sorted([f for f in os.listdir(image_dir)])\n",
    "    num_known = int(known_unknown_split * len(pig_folders))\n",
    "    np.random.shuffle(pig_folders)\n",
    "    known_pigs = pig_folders[:num_known]\n",
    "    unknown_pigs = pig_folders[num_known:]\n",
    "    return sorted(known_pigs), sorted(unknown_pigs)\n",
    "\n",
    "\n",
    "def prepare_known_splits(\n",
    "    filtered_dataset, known_classes, known_split=[0.7, 0.15, 0.15]\n",
    "):\n",
    "    np.random.seed(42)\n",
    "    train_samples, known_val_samples, known_test_samples = [], [], []\n",
    "    class_to_idx = {pig: i for i, pig in enumerate(known_classes)}\n",
    "    for pig in known_classes:\n",
    "        images = filtered_dataset[pig]\n",
    "        np.random.shuffle(images)\n",
    "\n",
    "        n_images = len(images)\n",
    "        n_train = int(known_split[0] * n_images)\n",
    "        adjusted_ratio = known_split[1] / (1 - known_split[0])\n",
    "        n_val = int(adjusted_ratio * (n_images - n_train))\n",
    "\n",
    "        train_images = images[:n_train]\n",
    "        val_images = images[n_train : n_train + n_val]\n",
    "        test_images = images[n_train + n_val :]\n",
    "\n",
    "        train_samples += [(img, class_to_idx[pig], pig) for img in train_images]\n",
    "        known_val_samples += [(img, class_to_idx[pig], pig) for img in val_images]\n",
    "        known_test_samples += [(img, class_to_idx[pig], pig) for img in test_images]\n",
    "\n",
    "    return train_samples, known_val_samples, known_test_samples\n",
    "\n",
    "\n",
    "def prepare_unknown_splits(filtered_dataset, unknown_classes, unknown_split=[0.5, 0.5]):\n",
    "    np.random.seed(42)\n",
    "    unknown_val_samples, unknown_test_samples = [], []\n",
    "    for pig in unknown_classes:\n",
    "        images = filtered_dataset[pig]\n",
    "        np.random.shuffle(images)\n",
    "\n",
    "        n_images = len(images)\n",
    "        n_val = int(unknown_split[0] * n_images)\n",
    "\n",
    "        val_images = images[:n_val]\n",
    "        test_images = images[n_val:]\n",
    "\n",
    "        unknown_val_samples += [(img, -1, pig) for img in val_images]\n",
    "        unknown_test_samples += [(img, -1, pig) for img in test_images]\n",
    "\n",
    "    return unknown_val_samples, unknown_test_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e49f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PigDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label, class_name = self.samples[idx]\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label, class_name if label != -1 else \"unknown\", img_path\n",
    "\n",
    "\n",
    "class PairDataset(Dataset):\n",
    "    def __init__(self, samples, num_pairs, transform=None):\n",
    "        self.samples = samples\n",
    "        self.num_pairs = num_pairs\n",
    "        self.transform = transform\n",
    "        self.class_name_to_indices = self._group_by_class_name()\n",
    "        self.pairs = self._create_pairs()\n",
    "\n",
    "    def _group_by_class_name(self):\n",
    "        class_name_to_indices = defaultdict(list)\n",
    "        for idx, (_, _, class_name) in enumerate(self.samples):\n",
    "            class_name_to_indices[class_name].append(idx)\n",
    "        return class_name_to_indices\n",
    "\n",
    "    def _create_pairs(self):\n",
    "        pairs = []\n",
    "        seen = set()\n",
    "        class_names = list(self.class_name_to_indices.keys())\n",
    "        num_pos = self.num_pairs // 2\n",
    "        num_neg = self.num_pairs - num_pos\n",
    "\n",
    "        generated_pos = 0\n",
    "        while generated_pos < num_pos:\n",
    "            class_name = np.random.choice(class_names)\n",
    "            indices = self.class_name_to_indices[class_name]\n",
    "            idx1, idx2 = np.random.choice(indices, 2, replace=False)\n",
    "            key = tuple(sorted((idx1, idx2)))\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            pairs.append((key, 1))\n",
    "            generated_pos += 1\n",
    "\n",
    "        generated_neg = 0\n",
    "        while generated_neg < num_neg:\n",
    "            class_name1, class_name2 = np.random.choice(class_names, 2, replace=False)\n",
    "            idx1 = np.random.choice(self.class_name_to_indices[class_name1])\n",
    "            idx2 = np.random.choice(self.class_name_to_indices[class_name2])\n",
    "            key = tuple(sorted((idx1, idx2)))\n",
    "            if key in seen:\n",
    "                continue\n",
    "            seen.add(key)\n",
    "            pairs.append((key, 0))\n",
    "            generated_neg += 1\n",
    "\n",
    "        np.random.shuffle(pairs)\n",
    "        return pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        (idx1, idx2), pair_label = self.pairs[idx]\n",
    "        img1_path, _, _ = self.samples[idx1]\n",
    "        img2_path, _, _ = self.samples[idx2]\n",
    "        img1 = cv2.cvtColor(cv2.imread(img1_path), cv2.COLOR_BGR2RGB)\n",
    "        img2 = cv2.cvtColor(cv2.imread(img2_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        return img1, img2, pair_label, img1_path, img2_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23690b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transforms(phase):\n",
    "    if phase == \"train\":\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.RandomResizedCrop(224, scale=(0.8, 1.0), ratio=(0.75, 1.33)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomVerticalFlip(p=0.2),\n",
    "                transforms.ColorJitter(\n",
    "                    brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "    return transform\n",
    "\n",
    "\n",
    "def get_dataloader(samples, phase, num_pairs=None):\n",
    "    transform = get_transforms(phase)\n",
    "    if num_pairs is not None:\n",
    "        dataset = PairDataset(samples, num_pairs, transform=transform)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, num_workers=4, pin_memory=True)\n",
    "    elif phase == \"train\":\n",
    "        labels = [label for _, label, _ in samples]\n",
    "        class_counts = Counter(labels)\n",
    "        num_samples = len(labels)\n",
    "        weights = [1.0 / class_counts[label] for label in labels]\n",
    "        sampler = WeightedRandomSampler(\n",
    "            weights, num_samples=num_samples, replacement=True\n",
    "        )\n",
    "        dataset = PigDataset(samples, transform=transform)\n",
    "        dataloader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=32,\n",
    "            sampler=sampler,\n",
    "            num_workers=4,\n",
    "            pin_memory=True,\n",
    "            drop_last=True,\n",
    "        )\n",
    "    else:\n",
    "        dataset = PigDataset(samples, transform=transform)\n",
    "        dataloader = DataLoader(dataset, batch_size=32, num_workers=4, pin_memory=True)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0ae2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(tensor):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    img = tensor.permute(1, 2, 0).numpy()\n",
    "    img = img * std + mean\n",
    "    img = np.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def plot_data_augmentation(\n",
    "    filtered_dataset, transform, save_path=\"data_augmentation.png\"\n",
    "):\n",
    "    classes = random.sample(list(filtered_dataset.keys()), 3)\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(8, 6))\n",
    "    for i, cls in enumerate(classes):\n",
    "        img_path = random.choice(filtered_dataset[cls])\n",
    "        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        axes[i, 0].imshow(img)\n",
    "        axes[i, 0].set_title(\"Original\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "\n",
    "        for j in range(3):\n",
    "            augmented = transform(img)\n",
    "            augmented_img = unnormalize(augmented)\n",
    "            axes[i, j + 1].imshow(augmented_img)\n",
    "            axes[i, j + 1].set_title(f\"Aug {j + 1}\")\n",
    "            axes[i, j + 1].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, save_path), dpi=600)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8431b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFaceHead(nn.Module):\n",
    "    def __init__(self, feature_dim, num_classes, s=30, m=0.5):\n",
    "        super().__init__()\n",
    "        self.scale, self.margin = s, m\n",
    "        self.weight = nn.Parameter(torch.Tensor(num_classes, feature_dim))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        x = F.normalize(features)\n",
    "        w = F.normalize(self.weight)\n",
    "        cos_theta = F.linear(x, w)\n",
    "        theta = torch.acos(torch.clamp(cos_theta, 1e-7 - 1.0, 1.0 - 1e-7))\n",
    "        target_cos = torch.cos(theta + self.margin)\n",
    "        one_hot = F.one_hot(labels, num_classes=self.weight.size(0)).float()\n",
    "        logits = cos_theta * (1 - one_hot) + target_cos * one_hot\n",
    "        return logits * self.scale\n",
    "\n",
    "\n",
    "class CenterLoss(nn.Module):\n",
    "    def __init__(self, num_classes, feature_dim, lambda_c=0.003):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.feature_dim = feature_dim\n",
    "        self.lambda_c = lambda_c\n",
    "        self.centers = nn.Parameter(torch.randn(num_classes, feature_dim))\n",
    "\n",
    "    def forward(self, features, labels):\n",
    "        centers_batch = self.centers.index_select(0, labels)\n",
    "        loss = F.mse_loss(features, centers_batch)\n",
    "        return self.lambda_c * loss\n",
    "\n",
    "\n",
    "class EfficientNetWithHeads(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim,\n",
    "        num_classes,\n",
    "        dropout=0.1,\n",
    "        s=30,\n",
    "        m=0.5,\n",
    "        lambda_c=0.003,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        backbone = timm.create_model(\n",
    "            \"efficientnet_b0\", pretrained=False, num_classes=0, global_pool=\"avg\"\n",
    "        )\n",
    "        backbone.load_state_dict(\n",
    "            torch.load(\n",
    "                os.path.join(SAVE_DIR, \"base_model.pt\"),\n",
    "                map_location=\"cpu\",\n",
    "                weights_only=True,\n",
    "            ),\n",
    "        )\n",
    "        blocks = list(backbone.blocks)\n",
    "        freeze_count = int(len(blocks) * 0.75)\n",
    "        for block in blocks[:freeze_count]:\n",
    "            for param in block.parameters():\n",
    "                param.requires_grad = False\n",
    "        self.backbone = backbone\n",
    "        in_features = backbone.num_features\n",
    "\n",
    "        layers = []\n",
    "        prev_dim = in_features\n",
    "        for _ in range(3):\n",
    "            layers.append(nn.Linear(prev_dim, 128, bias=False))\n",
    "            layers.append(nn.BatchNorm1d(128))\n",
    "            layers.append(nn.GELU())\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = 128\n",
    "        layers.append(nn.Linear(prev_dim, feature_dim))\n",
    "\n",
    "        self.feature_block = nn.Sequential(*layers)\n",
    "        self.arcface = ArcFaceHead(feature_dim, num_classes, s, m)\n",
    "        self.center_loss = CenterLoss(num_classes, feature_dim, lambda_c)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        feat = self.backbone(x)\n",
    "        feat = self.feature_block(feat)\n",
    "        feat = F.normalize(feat)\n",
    "        if labels is not None:\n",
    "            logits = self.arcface(feat, labels)\n",
    "            c_loss = self.center_loss(feat, labels)\n",
    "            return logits, feat, c_loss\n",
    "        return feat\n",
    "\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=10):\n",
    "        self.patience = patience\n",
    "        self.best_score = None\n",
    "        self.counter = 0\n",
    "\n",
    "    def __call__(self, current_score):\n",
    "        if self.best_score is None or current_score > self.best_score:\n",
    "            self.best_score = current_score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "        return self.counter >= self.patience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d9105d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, optimizer_center, device):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for imgs, labels, _, _ in loader:\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        optimizer_center.zero_grad()\n",
    "        logits, _, c_loss = model(imgs, labels)\n",
    "        ce = F.cross_entropy(logits, labels)\n",
    "        loss = ce + c_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer_center.step()\n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        pred = torch.argmax(logits, dim=1)\n",
    "        correct += (pred == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    return total_loss / len(loader.dataset), correct / total\n",
    "\n",
    "\n",
    "def validate(model, loader, device):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels, _, _ in loader:\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits, _, c_loss = model(imgs, labels)\n",
    "            ce = F.cross_entropy(logits, labels)\n",
    "            loss = ce + c_loss\n",
    "            total_loss += loss.item() * imgs.size(0)\n",
    "            pred = torch.argmax(logits, dim=1)\n",
    "            preds.extend(pred.cpu().tolist())\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    return total_loss / len(loader.dataset), correct / total\n",
    "\n",
    "\n",
    "def train_model(train_loader, val_loader, num_known, device, best, save=False):\n",
    "    model = EfficientNetWithHeads(\n",
    "        best[\"feat_dim\"],\n",
    "        num_known,\n",
    "        best[\"dropout\"],\n",
    "        best[\"s\"],\n",
    "        best[\"m\"],\n",
    "        best[\"lambda_c\"],\n",
    "    ).to(device)\n",
    "\n",
    "    params = (\n",
    "        list(model.backbone.parameters())\n",
    "        + list(model.feature_block.parameters())\n",
    "        + list(model.arcface.parameters())\n",
    "    )\n",
    "    opt = AdamW(params, lr=best[\"lr\"], weight_decay=best[\"wd\"])\n",
    "    opt_center = SGD(\n",
    "        model.center_loss.parameters(),\n",
    "        lr=best[\"lr_center\"],\n",
    "        momentum=best[\"momentum_center\"],\n",
    "        weight_decay=best[\"wd_center\"],\n",
    "    )\n",
    "    sched = CosineAnnealingWarmRestarts(opt, T_0=best[\"T_0\"], eta_min=best[\"eta_min\"])\n",
    "    sched_center = StepLR(\n",
    "        opt_center, step_size=best[\"step_size_center\"], gamma=best[\"gamma_center\"]\n",
    "    )\n",
    "\n",
    "    best_acc = 0.0\n",
    "    early_stopping = EarlyStopping()\n",
    "    best_model = copy.deepcopy(model.state_dict())\n",
    "    history = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "    }\n",
    "\n",
    "    for epoch in range(100):\n",
    "        train_loss, train_acc = train(model, train_loader, opt, opt_center, device)\n",
    "        val_loss, val_acc = validate(model, val_loader, device)\n",
    "        sched.step()\n",
    "        sched_center.step()\n",
    "\n",
    "        history[\"epoch\"].append(epoch + 1)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        history[\"train_acc\"].append(train_acc)\n",
    "        history[\"val_loss\"].append(val_loss)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        if early_stopping(val_acc):\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "\n",
    "    if not save:\n",
    "        return model\n",
    "\n",
    "    torch.save(model.state_dict(), os.path.join(SAVE_DIR, \"best_model.pt\"))\n",
    "    df = pd.DataFrame(history)\n",
    "    df.rename(\n",
    "        columns={\n",
    "            \"epoch\": \"Epoch\",\n",
    "            \"train_loss\": \"Train Loss\",\n",
    "            \"train_acc\": \"Train Accuracy\",\n",
    "            \"val_loss\": \"Validation Loss\",\n",
    "            \"val_acc\": \"Validation Accuracy\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    df.index = range(1, len(df) + 1)\n",
    "    df.index.name = \"Epoch\"\n",
    "    df.to_csv(os.path.join(SAVE_DIR, \"training_history.csv\"), index=False)\n",
    "    save_line_panel(history, \"training_history.png\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545c0526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_embeddings(model, device, loader):\n",
    "    model.eval()\n",
    "    embeddings = defaultdict(list)\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _, _ in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            feats = model(images)\n",
    "            feats = F.normalize(feats)\n",
    "            for feat, label in zip(feats, labels):\n",
    "                embeddings[label.item()].append(feat)\n",
    "    for label, feats in embeddings.items():\n",
    "        embeddings[label] = torch.stack(feats, dim=0)\n",
    "    embeddings = dict(sorted(embeddings.items()))\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def plot_tsne(gallery, label_to_class, save_path=\"tsne_gallery.png\"):\n",
    "    all_embeddings = []\n",
    "    all_labels = []\n",
    "\n",
    "    for label, feats in gallery.items():\n",
    "        all_embeddings.append(feats.cpu().numpy())\n",
    "        all_labels.extend([label] * feats.size(0))\n",
    "\n",
    "    all_embeddings = np.vstack(all_embeddings)\n",
    "    all_labels = np.array(all_labels)\n",
    "\n",
    "    tsne = TSNE(n_components=2, perplexity=5, init=\"random\", random_state=42)\n",
    "    tsne_result = tsne.fit_transform(all_embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    palette = sns.color_palette(\"hsv\", len(gallery))\n",
    "    for label in sorted(gallery.keys()):\n",
    "        idx = all_labels == label\n",
    "        plt.scatter(\n",
    "            tsne_result[idx, 0],\n",
    "            tsne_result[idx, 1],\n",
    "            label=label_to_class[label],\n",
    "            s=60,\n",
    "            alpha=0.7,\n",
    "            color=palette[label],\n",
    "        )\n",
    "\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc=\"upper left\")\n",
    "    plt.title(\"t-SNE Plot of Prototype Embeddings\")\n",
    "    plt.xlabel(\"t-SNE 1\")\n",
    "    plt.ylabel(\"t-SNE 2\")\n",
    "    plt.tight_layout()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(SAVE_DIR, save_path), dpi=600)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def build_prototype_gallery(\n",
    "    embeddings, num_prototypes=3, known_classes=None, save=False\n",
    "):\n",
    "    gallery = {}\n",
    "    for label, feats in embeddings.items():\n",
    "        feats = feats.cpu().numpy()\n",
    "        n_samples, _ = feats.shape\n",
    "        k = min(num_prototypes, n_samples)\n",
    "        km = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "        km.fit(feats)\n",
    "        centers = km.cluster_centers_\n",
    "        prototypes = torch.from_numpy(centers)\n",
    "        gallery[label] = F.normalize(prototypes)\n",
    "\n",
    "    if not save:\n",
    "        return gallery\n",
    "\n",
    "    label_to_class = {i: known_classes[i] for i in range(len(known_classes))}\n",
    "    plot_tsne(gallery, label_to_class)\n",
    "    with open(os.path.join(SAVE_DIR, \"feature_gallery.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                \"gallery\": gallery,\n",
    "                \"label_to_class\": label_to_class,\n",
    "            },\n",
    "            f,\n",
    "        )\n",
    "    return gallery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5f8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(model, gallery, loader, device):\n",
    "    model.eval()\n",
    "    scores = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, _, _, _ in loader:\n",
    "            imgs = imgs.to(device)\n",
    "            feats = F.normalize(model(imgs.to(device)))\n",
    "            feats = feats.cpu().numpy()\n",
    "            for f in feats:\n",
    "                score = max([(protos @ f).max().item() for protos in gallery.values()])\n",
    "                scores.append(score)\n",
    "    return np.array(scores)\n",
    "\n",
    "\n",
    "def tune_osr_threshold(model, gallery, known_loader, unknown_loader, device, plot=True):\n",
    "    known_scores = compute_scores(model, gallery, known_loader, device)\n",
    "    unknown_scores = compute_scores(model, gallery, unknown_loader, device)\n",
    "\n",
    "    y_true = np.concatenate([np.ones_like(known_scores), np.zeros_like(unknown_scores)])\n",
    "    y_scores = np.concatenate([known_scores, unknown_scores])\n",
    "\n",
    "    thresholds = np.unique(y_scores)\n",
    "    f1s = np.array(\n",
    "        [f1_score(y_true, (y_scores >= thr).astype(int)) for thr in thresholds]\n",
    "    )\n",
    "\n",
    "    best_idx = np.argmax(f1s)\n",
    "    best_thresh = thresholds[best_idx]\n",
    "    best_f1 = f1s[best_idx]\n",
    "\n",
    "    if not plot:\n",
    "        return best_thresh\n",
    "\n",
    "    def plot_thres(ax):\n",
    "        ax.plot(thresholds, f1s, linewidth=2)\n",
    "\n",
    "    save_square(\n",
    "        plot_thres,\n",
    "        \"osr_threshold.png\",\n",
    "        \"Open-Set Recognition: F1 vs Threshold\",\n",
    "        \"Threshold\",\n",
    "        \"F1 Score\",\n",
    "        annotate=f\"Best={best_f1:.3f} @ Thr={best_thresh:.3f}\",\n",
    "    )\n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4461846",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pair_scores(model, loader, device):\n",
    "    model.eval()\n",
    "    sims, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, lbl, _, _ in loader:\n",
    "            e1 = F.normalize(model(img1.to(device)))\n",
    "            e2 = F.normalize(model(img2.to(device)))\n",
    "            sim = (e1 * e2).sum(dim=1).cpu().numpy()\n",
    "            sims.append(sim)\n",
    "            labels.append(lbl.numpy())\n",
    "    return np.concatenate(sims), np.concatenate(labels)\n",
    "\n",
    "\n",
    "def tune_pair_threshold(model, pair_loader, device, plot=True):\n",
    "    sims, labels = compute_pair_scores(model, pair_loader, device)\n",
    "\n",
    "    thresholds = np.unique(sims)\n",
    "    f1s = np.array([f1_score(labels, (sims >= thr).astype(int)) for thr in thresholds])\n",
    "\n",
    "    best_idx = np.argmax(f1s)\n",
    "    best_thresh = thresholds[best_idx]\n",
    "    best_f1 = f1s[best_idx]\n",
    "\n",
    "    best_thresh = float(best_thresh)\n",
    "\n",
    "    if not plot:\n",
    "        return best_thresh\n",
    "\n",
    "    def plot_thres(ax):\n",
    "        ax.plot(thresholds, f1s, linewidth=2)\n",
    "\n",
    "    save_square(\n",
    "        plot_thres,\n",
    "        \"verif_threshold.png\",\n",
    "        \"Pair Verification: F1 vs Threshold\",\n",
    "        \"Similarity Threshold\",\n",
    "        \"F1 Score\",\n",
    "        annotate=f\"Best={best_f1:.3f} @ Thr={best_thresh:.3f}\",\n",
    "    )\n",
    "    return best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe10b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_osr_identification(\n",
    "    model, gallery, known_loader, unknown_loader, device, threshold, plot=True\n",
    "):\n",
    "    model.eval()\n",
    "    records = []\n",
    "    label_to_class = {}\n",
    "\n",
    "    def process_batch(imgs):\n",
    "        imgs = imgs.to(device)\n",
    "        feats = F.normalize(model(imgs), dim=1)\n",
    "        sims = (\n",
    "            torch.stack(\n",
    "                [\n",
    "                    (feats @ proto.to(device).T).max(dim=1)[0]\n",
    "                    for proto in gallery.values()\n",
    "                ],\n",
    "                dim=1,\n",
    "            )\n",
    "            .detach()\n",
    "            .cpu()\n",
    "            .numpy()\n",
    "        )\n",
    "        max_sims = sims.max(axis=1)\n",
    "        argmax_ids = sims.argmax(axis=1)\n",
    "        return max_sims, argmax_ids\n",
    "\n",
    "    for imgs, labels, class_names, paths in known_loader:\n",
    "        sim, argm = process_batch(imgs)\n",
    "        for s, a, true, p in zip(sim, argm, labels.numpy(), paths):\n",
    "            records.append(\n",
    "                {\"score\": s, \"argmax\": int(a), \"true\": int(true), \"img_path\": p}\n",
    "            )\n",
    "        for label, class_name in zip(labels.numpy(), class_names):\n",
    "            if label not in label_to_class:\n",
    "                label_to_class[label] = class_name\n",
    "\n",
    "    label_to_class = dict(sorted(label_to_class.items()))\n",
    "    label_to_class[-1] = \"unknown\"\n",
    "\n",
    "    for imgs, _, _, paths in unknown_loader:\n",
    "        sim, argm = process_batch(imgs)\n",
    "        for s, a, p in zip(sim, argm, paths):\n",
    "            records.append({\"score\": s, \"argmax\": int(a), \"true\": -1, \"img_path\": p})\n",
    "\n",
    "    scores = np.array([r[\"score\"] for r in records])\n",
    "    argmaxes = np.array([r[\"argmax\"] for r in records])\n",
    "    trues = np.array([r[\"true\"] for r in records])\n",
    "    paths = [r[\"img_path\"] for r in records]\n",
    "\n",
    "    preds = np.where(scores >= threshold, argmaxes, -1)\n",
    "\n",
    "    acc_c = accuracy_score(trues[trues != -1], preds[trues != -1])\n",
    "    acc_oc = accuracy_score(trues, preds)\n",
    "    prec_macro = precision_score(trues, preds, average=\"macro\", zero_division=0)\n",
    "    rec_macro = recall_score(trues, preds, average=\"macro\", zero_division=0)\n",
    "    prec_micro = precision_score(trues, preds, average=\"micro\", zero_division=0)\n",
    "    rec_micro = recall_score(trues, preds, average=\"micro\", zero_division=0)\n",
    "    f1_macro = f1_score(trues, preds, average=\"macro\", zero_division=0)\n",
    "    f1_micro = f1_score(trues, preds, average=\"micro\", zero_division=0)\n",
    "\n",
    "    labels_list = list(gallery.keys()) + [-1]\n",
    "    p, r, f1, sup = precision_recall_fscore_support(\n",
    "        trues, preds, labels=labels_list, zero_division=0\n",
    "    )\n",
    "    per_cls = {}\n",
    "    for i, cls in enumerate(labels_list):\n",
    "        per_cls[label_to_class[cls]] = {\n",
    "            \"accuracy\": (\n",
    "                (preds[trues == cls] == trues[trues == cls]).mean() if sup[i] > 0 else 0\n",
    "            ),\n",
    "            \"precision\": p[i],\n",
    "            \"recall\": r[i],\n",
    "            \"f1\": f1[i],\n",
    "            \"support\": sup[i],\n",
    "            \"misclassified\": (preds[trues == cls] != trues[trues == cls]).sum(),\n",
    "        }\n",
    "\n",
    "    y_det_true = (trues != -1).astype(int)\n",
    "    y_det_scores = scores\n",
    "    osr_auroc = roc_auc_score(y_det_true, y_det_scores)\n",
    "    prec_curve, rec_curve, _ = precision_recall_curve(y_det_true, scores)\n",
    "    osr_aupr = auc(rec_curve, prec_curve)\n",
    "    y_det_pred = (y_det_scores >= threshold).astype(int)\n",
    "    osr_acc = accuracy_score(y_det_true, y_det_pred)\n",
    "    osr_prec = precision_score(y_det_true, y_det_pred, zero_division=0)\n",
    "    osr_rec = recall_score(y_det_true, y_det_pred, zero_division=0)\n",
    "    osr_f1 = f1_score(y_det_true, y_det_pred, zero_division=0)\n",
    "    far = ((y_det_true == 0) & (y_det_pred == 1)).sum() / (y_det_true == 0).sum()\n",
    "    ccr = accuracy_score(trues, preds)\n",
    "    oscr = (1 - far) * ccr\n",
    "\n",
    "    metrics = {\n",
    "        \"identification_accuracy_closed_set\": acc_c,\n",
    "        \"identification_accuracy_open_closed_set\": acc_oc,\n",
    "        \"identification_precision_macro_avg\": prec_macro,\n",
    "        \"identification_recall_macro_avg\": rec_macro,\n",
    "        \"identification_f1_macro_avg\": f1_macro,\n",
    "        \"identification_precision_micro_avg\": prec_micro,\n",
    "        \"identification_recall_micro_avg\": rec_micro,\n",
    "        \"identification_f1_micro_avg\": f1_micro,\n",
    "        \"identification_per_class_metrics\": per_cls,\n",
    "        \"open_set_accuracy\": osr_acc,\n",
    "        \"open_set_precision\": osr_prec,\n",
    "        \"open_set_recall\": osr_rec,\n",
    "        \"open_set_f1_score\": osr_f1,\n",
    "        \"open_set_auroc\": osr_auroc,\n",
    "        \"open_set_aupr\": osr_aupr,\n",
    "        \"open_set_false_accept_rate\": far,\n",
    "        \"open_set_correct_classification_rate\": ccr,\n",
    "        \"open_set_classification_rate\": oscr,\n",
    "    }\n",
    "\n",
    "    miscls = []\n",
    "    for t, p, s, path in zip(trues, preds, scores, paths):\n",
    "        if t != p:\n",
    "            miscls.append(\n",
    "                {\n",
    "                    \"img_path\": path,\n",
    "                    \"true\": label_to_class[t],\n",
    "                    \"pred\": label_to_class[p],\n",
    "                    \"score\": float(s),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    if not plot:\n",
    "        return metrics, miscls\n",
    "\n",
    "    cm = confusion_matrix(trues, preds, labels=labels_list)\n",
    "    with plt.rc_context(rc=None):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        ax = sns.heatmap(\n",
    "            cm,\n",
    "            annot=False,\n",
    "            fmt=\"g\",\n",
    "            cmap=\"Blues\",\n",
    "            cbar=True,\n",
    "            square=True,\n",
    "            xticklabels=label_to_class.values(),\n",
    "            yticklabels=label_to_class.values(),\n",
    "        )\n",
    "        ax.tick_params(axis=\"x\", rotation=90, labelsize=12)\n",
    "        ax.tick_params(axis=\"y\", rotation=0, labelsize=12)\n",
    "        ax.set_xlabel(\"Predicted\", fontsize=16)\n",
    "        ax.set_ylabel(\"True\", fontsize=16)\n",
    "        ax.set_title(\"Identification Confusion Matrix\", fontsize=18)\n",
    "        plt.grid(False)\n",
    "        plt.tight_layout()\n",
    "        fullpath = os.path.join(SAVE_DIR, \"id_confusion_matrix.png\")\n",
    "        plt.savefig(fullpath, dpi=600)\n",
    "        plt.close()\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(y_det_true, y_det_scores)\n",
    "\n",
    "    def plot_osr_roc(ax):\n",
    "        ax.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
    "        ax.plot([0, 1], [0, 1], \"k--\", label=\"Chance\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "    def plot_osr_pr(ax):\n",
    "        ax.plot(rec_curve, prec_curve, linewidth=2)\n",
    "\n",
    "    save_square(\n",
    "        plot_osr_roc,\n",
    "        \"osr_roc.png\",\n",
    "        title=f\"OSR ROC (AUC={osr_auroc:.3f})\",\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        annotate=f\"AUC={osr_auroc:.3f}\",\n",
    "    )\n",
    "    save_square(\n",
    "        plot_osr_pr,\n",
    "        \"osr_pr.png\",\n",
    "        title=f\"OSR PR (AUC={osr_aupr:.3f})\",\n",
    "        xlabel=\"Recall\",\n",
    "        ylabel=\"Precision\",\n",
    "    )\n",
    "    save_dual_hist(\n",
    "        y_det_scores[y_det_true == 1],\n",
    "        y_det_scores[y_det_true == 0],\n",
    "        \"osr_similarity_score.png\",\n",
    "        \"OSR Similarity Distributions\",\n",
    "        [\"Known\", \"Unknown\"],\n",
    "        threshold,\n",
    "    )\n",
    "\n",
    "    return metrics, miscls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a947b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_verification(model, pair_loader, device, threshold, plot=True):\n",
    "    model.eval()\n",
    "    sims = []\n",
    "    labels = []\n",
    "    paths1, paths2 = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img1, img2, lbl, p1, p2 in pair_loader:\n",
    "            e1 = F.normalize(model(img1.to(device)), dim=1)\n",
    "            e2 = F.normalize(model(img2.to(device)), dim=1)\n",
    "            sim = (e1 * e2).sum(dim=1).cpu().numpy()\n",
    "            sims.append(sim)\n",
    "            labels.append(lbl.numpy())\n",
    "            paths1.extend(p1)\n",
    "            paths2.extend(p2)\n",
    "\n",
    "    sims = np.concatenate(sims)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "    preds = (sims >= threshold).astype(int)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    prec = precision_score(labels, preds, zero_division=0)\n",
    "    rec = recall_score(labels, preds, zero_division=0)\n",
    "    f1 = f1_score(labels, preds, zero_division=0)\n",
    "    auroc = roc_auc_score(labels, sims)\n",
    "    prec_curve, rec_curve, _ = precision_recall_curve(labels, sims)\n",
    "    aupr = auc(rec_curve, prec_curve)\n",
    "    avg_prec = average_precision_score(labels, sims)\n",
    "    fpr, tpr, _ = roc_curve(labels, sims)\n",
    "    fnr = 1 - tpr\n",
    "    eer_idx = np.nanargmin(np.abs(fpr - fnr))\n",
    "    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
    "    far = ((labels == 0) & (preds == 1)).sum() / (labels == 0).sum()\n",
    "    frr = ((labels == 1) & (preds == 0)).sum() / (labels == 1).sum()\n",
    "\n",
    "    false_rejects = []\n",
    "    false_accepts = []\n",
    "    for s, y, yhat, p1, p2 in zip(sims, labels, preds, paths1, paths2):\n",
    "        if y == 1 and yhat == 0:\n",
    "            false_rejects.append({\"path1\": p1, \"path2\": p2, \"score\": float(s)})\n",
    "        elif y == 0 and yhat == 1:\n",
    "            false_accepts.append({\"path1\": p1, \"path2\": p2, \"score\": float(s)})\n",
    "\n",
    "    report = {\"false_rejects\": false_rejects, \"false_accepts\": false_accepts}\n",
    "\n",
    "    metrics = {\n",
    "        \"verification_accuracy\": acc,\n",
    "        \"verification_precision\": prec,\n",
    "        \"verification_recall\": rec,\n",
    "        \"verification_f1_score\": f1,\n",
    "        \"verification_auroc\": auroc,\n",
    "        \"verification_aupr\": aupr,\n",
    "        \"verification_average_precision\": avg_prec,\n",
    "        \"verification_equal_error_rate\": eer,\n",
    "        \"verification_false_acceptance_rate\": far,\n",
    "        \"verification_false_rejection_rate\": frr,\n",
    "    }\n",
    "\n",
    "    if not plot:\n",
    "        return metrics, report\n",
    "\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    def plot_verif_cm(ax):\n",
    "        ax.imshow(cm, cmap=plt.cm.Blues, interpolation=\"nearest\")\n",
    "        ax.set_xticks([0, 1])\n",
    "        ax.set_xticklabels([\"Impostor\", \"Genuine\"], rotation=0)\n",
    "        ax.set_yticks([0, 1])\n",
    "        ax.set_yticklabels([\"Impostor\", \"Genuine\"], rotation=90)\n",
    "        ax.grid(False)\n",
    "        for i in range(2):\n",
    "            for j in range(2):\n",
    "                ax.text(\n",
    "                    j,\n",
    "                    i,\n",
    "                    cm[i, j],\n",
    "                    ha=\"center\",\n",
    "                    va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\",\n",
    "                )\n",
    "\n",
    "    def plot_verif_roc(ax):\n",
    "        ax.plot(fpr, tpr, linewidth=2, label=\"ROC curve\")\n",
    "        ax.plot([0, 1], [0, 1], \"k--\", label=\"Chance\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "    def plot_verif_pr(ax):\n",
    "        ax.plot(rec_curve, prec_curve, linewidth=2)\n",
    "\n",
    "    def plot_det(ax):\n",
    "        ax.plot(fpr, fnr, linewidth=2, label=\"DET Curve\")\n",
    "        ax.plot([0, 1], [0, 1], \"k--\", label=\"EER Line\")\n",
    "        ax.scatter(fpr[eer_idx], fnr[eer_idx], color=\"red\", label=f\"EER={eer:.3f}\")\n",
    "        ax.legend(loc=\"lower right\")\n",
    "\n",
    "    save_square(\n",
    "        plot_verif_cm,\n",
    "        \"verif_confusion_matrix.png\",\n",
    "        title=\"Verification Confusion Matrix\",\n",
    "        xlabel=\"Predicted\",\n",
    "        ylabel=\"True\",\n",
    "    )\n",
    "    save_square(\n",
    "        plot_verif_roc,\n",
    "        \"verif_roc.png\",\n",
    "        title=f\"Verification ROC (AUC={auroc:.3f})\",\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        annotate=f\"AUC={auroc:.3f}\",\n",
    "    )\n",
    "    save_square(\n",
    "        plot_verif_pr,\n",
    "        \"verif_pr.png\",\n",
    "        title=f\"Verification PR (AUC={aupr:.3f})\",\n",
    "        xlabel=\"Recall\",\n",
    "        ylabel=\"Precision\",\n",
    "    )\n",
    "    save_square(\n",
    "        plot_det,\n",
    "        \"verif_eer_curve.png\",\n",
    "        title=\"DET Curve\",\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"False Negative Rate\",\n",
    "    )\n",
    "    save_dual_hist(\n",
    "        sims[labels == 1],\n",
    "        sims[labels == 0],\n",
    "        \"verif_similarity_score.png\",\n",
    "        \"Verification Similarity Distributions\",\n",
    "        [\"Genuine\", \"Impostor\"],\n",
    "        threshold,\n",
    "    )\n",
    "\n",
    "    return metrics, report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e02c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(\n",
    "    filtered_dataset,\n",
    "    known_classes,\n",
    "    unknown_val,\n",
    "    unknown_test,\n",
    "    num_known,\n",
    "    device,\n",
    "):\n",
    "    def objective(trial):\n",
    "        trial.suggest_categorical(\"feat_dim\", [256, 384, 512])\n",
    "        trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "        trial.suggest_int(\"s\", 45, 65)\n",
    "        trial.suggest_float(\"m\", 0.35, 0.45)\n",
    "        trial.suggest_float(\"lambda_c\", 1e-4, 1e-1)\n",
    "\n",
    "        trial.suggest_float(\"lr\", 1e-5, 5e-4, log=True)\n",
    "        trial.suggest_float(\"wd\", 1e-6, 1e-3, log=True)\n",
    "\n",
    "        trial.suggest_float(\"lr_center\", 1e-5, 5e-4, log=True)\n",
    "        trial.suggest_float(\"wd_center\", 1e-6, 1e-3, log=True)\n",
    "        trial.suggest_float(\"momentum_center\", 0.8, 0.95)\n",
    "\n",
    "        trial.suggest_int(\"T_0\", 3, 10)\n",
    "        trial.suggest_float(\"eta_min\", 1e-6, 1e-4)\n",
    "\n",
    "        trial.suggest_int(\"step_size_center\", 5, 20)\n",
    "        trial.suggest_float(\"gamma_center\", 0.1, 0.9)\n",
    "\n",
    "        scores = []\n",
    "        inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=24)\n",
    "        for inner_idx, (train_idx, val_idx) in enumerate(\n",
    "            inner_cv.split(outer_train, outer_train_labels)\n",
    "        ):\n",
    "            inner_train = [outer_train[i] for i in train_idx]\n",
    "            inner_val = [outer_train[i] for i in val_idx]\n",
    "\n",
    "            train_loader = get_dataloader(inner_train, \"train\")\n",
    "            known_val_loader = get_dataloader(inner_val, \"val\")\n",
    "            pair_val_loader = get_dataloader(\n",
    "                inner_val + unknown_val, \"val\", num_pairs=10000\n",
    "            )\n",
    "            unknown_val_loader = get_dataloader(unknown_val, \"val\")\n",
    "\n",
    "            model = train_model(\n",
    "                train_loader, known_val_loader, num_known, device, trial.params\n",
    "            )\n",
    "            gallery = build_prototype_gallery(\n",
    "                extract_embeddings(model, device, train_loader), num_prototypes=3\n",
    "            )\n",
    "\n",
    "            osr_threshold = tune_osr_threshold(\n",
    "                model,\n",
    "                gallery,\n",
    "                known_val_loader,\n",
    "                unknown_val_loader,\n",
    "                device,\n",
    "                plot=False,\n",
    "            )\n",
    "            verif_threshold = tune_pair_threshold(\n",
    "                model, pair_val_loader, device, plot=False\n",
    "            )\n",
    "\n",
    "            metrics_osr_id, _ = evaluate_osr_identification(\n",
    "                model,\n",
    "                gallery,\n",
    "                known_val_loader,\n",
    "                unknown_val_loader,\n",
    "                device,\n",
    "                osr_threshold,\n",
    "                plot=False,\n",
    "            )\n",
    "            metrics_verif, _ = evaluate_verification(\n",
    "                model, pair_val_loader, device, verif_threshold, plot=False\n",
    "            )\n",
    "\n",
    "            score = (\n",
    "                0.34 * metrics_osr_id[\"identification_accuracy_closed_set\"]\n",
    "                + 0.33 * metrics_osr_id[\"open_set_auroc\"]\n",
    "                + 0.33 * metrics_verif[\"verification_auroc\"]\n",
    "            )\n",
    "            scores.append(score)\n",
    "\n",
    "            trial.report(score, inner_idx)\n",
    "            if trial.should_prune():\n",
    "                raise optuna.TrialPruned()\n",
    "\n",
    "        return np.mean(scores)\n",
    "\n",
    "    known_samples, labels = [], []\n",
    "    class_to_idx = {pig: i for i, pig in enumerate(known_classes)}\n",
    "    for pig in known_classes:\n",
    "        images = filtered_dataset[pig]\n",
    "        for img in images:\n",
    "            known_samples.append((img, class_to_idx[pig], pig))\n",
    "            labels.append(class_to_idx[pig])\n",
    "\n",
    "    trials = []\n",
    "    outer_metrics = []\n",
    "    best_params_list = []\n",
    "    outer_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for outer_idx, (outer_train_idx, outer_test_idx) in enumerate(\n",
    "        outer_cv.split(known_samples, labels)\n",
    "    ):\n",
    "        outer_train = [known_samples[i] for i in outer_train_idx]\n",
    "        outer_train_labels = [labels[i] for i in outer_train_idx]\n",
    "        outer_test = [known_samples[i] for i in outer_test_idx]\n",
    "\n",
    "        sampler = TPESampler()\n",
    "        pruner = MedianPruner(n_startup_trials=10)\n",
    "        study = optuna.create_study(\n",
    "            study_name=f\"Model Hyperparameter Tuning: Fold {outer_idx + 1}\",\n",
    "            direction=\"maximize\",\n",
    "            sampler=sampler,\n",
    "            pruner=pruner,\n",
    "        )\n",
    "        study.optimize(objective, n_trials=100)\n",
    "\n",
    "        trial_df = study.trials_dataframe()\n",
    "        trial_df[\"fold\"] = f\"Fold {outer_idx + 1}\"\n",
    "        trials.append(trial_df)\n",
    "        best_params_list.append((study.best_trial.params, study.best_trial.value))\n",
    "        best = study.best_trial.params\n",
    "\n",
    "        train_loader = get_dataloader(outer_train, \"train\")\n",
    "        known_test_loader = get_dataloader(outer_test, \"test\")\n",
    "        unknown_test_loader = get_dataloader(unknown_test, \"test\")\n",
    "        pair_test_loader = get_dataloader(\n",
    "            outer_test + unknown_test, \"test\", num_pairs=20000\n",
    "        )\n",
    "\n",
    "        model = train_model(train_loader, known_test_loader, num_known, device, best)\n",
    "        gallery = build_prototype_gallery(\n",
    "            extract_embeddings(model, device, train_loader), num_prototypes=3\n",
    "        )\n",
    "\n",
    "        osr_threshold = tune_osr_threshold(\n",
    "            model, gallery, known_test_loader, unknown_test_loader, device, plot=False\n",
    "        )\n",
    "        verif_threshold = tune_pair_threshold(\n",
    "            model, pair_test_loader, device, plot=False\n",
    "        )\n",
    "\n",
    "        metrics_osr_id, _ = evaluate_osr_identification(\n",
    "            model,\n",
    "            gallery,\n",
    "            known_test_loader,\n",
    "            unknown_test_loader,\n",
    "            device,\n",
    "            osr_threshold,\n",
    "            plot=False,\n",
    "        )\n",
    "        metrics_verif, _ = evaluate_verification(\n",
    "            model, pair_test_loader, device, verif_threshold, plot=False\n",
    "        )\n",
    "\n",
    "        metrics = {}\n",
    "        for key, value in metrics_osr_id.items():\n",
    "            if key == \"identification_per_class_metrics\":\n",
    "                metrics[\"osr_decision_threshold\"] = osr_threshold\n",
    "                continue\n",
    "            metrics[key] = value\n",
    "        metrics[\"verification_decision_threshold\"] = verif_threshold\n",
    "        for key, value in metrics_verif.items():\n",
    "            metrics[key] = value\n",
    "\n",
    "        outer_metrics.append(metrics)\n",
    "\n",
    "    df = pd.DataFrame(outer_metrics)\n",
    "    rename_dict = {\n",
    "        \"osr_decision_threshold\": \"Open-Set Decision Threshold\",\n",
    "        \"verification_decision_threshold\": \"Verification Decision Threshold\",\n",
    "        \"identification_accuracy_closed_set\": \"Identification Accuracy Closed Set\",\n",
    "        \"identification_accuracy_open_closed_set\": \"Identification Accuracy Open Closed Set\",\n",
    "        \"identification_precision_macro_avg\": \"Identification Precision Macro Avg\",\n",
    "        \"identification_recall_macro_avg\": \"Identification Recall Macro Avg\",\n",
    "        \"identification_f1_macro_avg\": \"Identification F1 Macro Avg\",\n",
    "        \"identification_precision_micro_avg\": \"Identification Precision Micro Avg\",\n",
    "        \"identification_recall_micro_avg\": \"Identification Recall Micro Avg\",\n",
    "        \"identification_f1_micro_avg\": \"Identification F1 Micro Avg\",\n",
    "        \"open_set_accuracy\": \"Open-Set Accuracy\",\n",
    "        \"open_set_precision\": \"Open-Set Precision\",\n",
    "        \"open_set_recall\": \"Open-Set Recall\",\n",
    "        \"open_set_f1_score\": \"Open-Set F1 Score\",\n",
    "        \"open_set_auroc\": \"Open-Set AUROC\",\n",
    "        \"open_set_aupr\": \"Open-Set AUPR\",\n",
    "        \"open_set_false_accept_rate\": \"Open-Set False Accept Rate\",\n",
    "        \"open_set_correct_classification_rate\": \"Open-Set Correct Classification Rate\",\n",
    "        \"open_set_classification_rate\": \"Open-Set Classification Rate\",\n",
    "        \"verification_accuracy\": \"Verification Accuracy\",\n",
    "        \"verification_precision\": \"Verification Precision\",\n",
    "        \"verification_recall\": \"Verification Recall\",\n",
    "        \"verification_f1_score\": \"Verification F1 Score\",\n",
    "        \"verification_auroc\": \"Verification AUROC\",\n",
    "        \"verification_aupr\": \"Verification AUPR\",\n",
    "        \"verification_average_precision\": \"Verification Average Precision\",\n",
    "        \"verification_equal_error_rate\": \"Verification Equal Error Rate\",\n",
    "        \"verification_false_acceptance_rate\": \"Verification False Acceptance Rate\",\n",
    "        \"verification_false_rejection_rate\": \"Verification False Rejection Rate\",\n",
    "    }\n",
    "    df.rename(\n",
    "        columns={k: v for k, v in rename_dict.items() if k in df.columns}, inplace=True\n",
    "    )\n",
    "    df.index = [f\"Fold {i}\" for i in range(1, len(df) + 1)]\n",
    "    df.index.name = \"Cross-Validation Fold\"\n",
    "    df.loc[\"Mean\"] = df.mean(numeric_only=True)\n",
    "    df.loc[\"Std\"] = df.std(numeric_only=True)\n",
    "    df.to_csv(os.path.join(SAVE_DIR, \"cv_results.csv\"))\n",
    "\n",
    "    rename_dict = {\n",
    "        \"fold\": \"Cross-Validation Fold\",\n",
    "        \"number\": \"Trial Number\",\n",
    "        \"value\": \"Score\",\n",
    "        \"params_feat_dim\": \"Feature Dimensions\",\n",
    "        \"params_dropout\": \"Dropout Rate\",\n",
    "        \"params_s\": \"ArcFace Scaling Factor\",\n",
    "        \"params_m\": \"ArcFace Angular Margin Penalty\",\n",
    "        \"params_lambda_c\": \"Lambda Center\",\n",
    "        \"params_lr\": \"Learning Rate\",\n",
    "        \"params_wd\": \"Weight Decay\",\n",
    "        \"params_lr_center\": \"Learning Rate Center\",\n",
    "        \"params_wd_center\": \"Weight Decay Center\",\n",
    "        \"params_momentum_center\": \"Momentum Center\",\n",
    "        \"params_T_0\": \"Scheduler T0\",\n",
    "        \"params_eta_min\": \"Scheduler Eta Min\",\n",
    "        \"params_step_size_center\": \"Scheduler Center Step Size\",\n",
    "        \"params_gamma_center\": \"Scheduler Center Gamma\",\n",
    "        \"state\": \"State\",\n",
    "    }\n",
    "    df = pd.concat(trials, ignore_index=True)\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "    df = df[[value for value in rename_dict.values() if value in df.columns]]\n",
    "    df.to_csv(os.path.join(SAVE_DIR, \"cv_params_trials.csv\"), index=False)\n",
    "\n",
    "    rename_dict = {\n",
    "        key[7:] if key.startswith(\"params_\") else key: value\n",
    "        for key, value in rename_dict.items()\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame([value[0] for value in best_params_list])\n",
    "    df.rename(columns=rename_dict, inplace=True)\n",
    "    df.index = [f\"CV Fold {i}\" for i in range(1, len(df) + 1)]\n",
    "    df.index.name = \"Cross-Validation Fold\"\n",
    "    df.insert(0, \"Score\", [value[1] for value in best_params_list])\n",
    "    df.to_csv(os.path.join(SAVE_DIR, \"best_params.csv\"))\n",
    "\n",
    "    best_params = max(best_params_list, key=lambda x: x[1])[0]\n",
    "    return best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4d6db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_model(\n",
    "    filtered_dataset,\n",
    "    known_classes,\n",
    "    unknown_val_samples,\n",
    "    unknown_test_samples,\n",
    "    num_known,\n",
    "    best_params,\n",
    "    device,\n",
    "):\n",
    "    train_samples, known_val_samples, known_test_samples = prepare_known_splits(\n",
    "        filtered_dataset, known_classes\n",
    "    )\n",
    "\n",
    "    train_loader = get_dataloader(train_samples, \"train\")\n",
    "    known_val_loader = get_dataloader(known_val_samples, \"val\")\n",
    "    known_test_loader = get_dataloader(known_test_samples, \"test\")\n",
    "    unknown_val_loader = get_dataloader(unknown_val_samples, \"val\")\n",
    "    unknown_test_loader = get_dataloader(unknown_test_samples, \"test\")\n",
    "    pair_val_loader = get_dataloader(\n",
    "        known_val_samples + unknown_val_samples, \"val\", num_pairs=10000\n",
    "    )\n",
    "    pair_test_loader = get_dataloader(\n",
    "        known_test_samples + unknown_test_samples, \"test\", num_pairs=20000\n",
    "    )\n",
    "\n",
    "    plot_data_augmentation(filtered_dataset, get_transforms(\"train\"))\n",
    "\n",
    "    model = train_model(\n",
    "        train_loader, known_val_loader, num_known, device, best_params, save=True\n",
    "    )\n",
    "    gallery = build_prototype_gallery(\n",
    "        extract_embeddings(model, device, train_loader),\n",
    "        num_prototypes=3,\n",
    "        known_classes=known_classes,\n",
    "        save=True,\n",
    "    )\n",
    "\n",
    "    osr_threshold = tune_osr_threshold(\n",
    "        model, gallery, known_val_loader, unknown_val_loader, device\n",
    "    )\n",
    "    verif_threshold = tune_pair_threshold(model, pair_val_loader, device)\n",
    "\n",
    "    metrics_osr_id, miscls = evaluate_osr_identification(\n",
    "        model, gallery, known_test_loader, unknown_test_loader, device, osr_threshold\n",
    "    )\n",
    "    metrics_verif, report = evaluate_verification(\n",
    "        model, pair_test_loader, device, verif_threshold\n",
    "    )\n",
    "\n",
    "    per_class_df = pd.DataFrame.from_dict(\n",
    "        metrics_osr_id[\"identification_per_class_metrics\"], orient=\"index\"\n",
    "    )\n",
    "    per_class_df.reset_index(inplace=True)\n",
    "    per_class_df.rename(columns={\"index\": \"class_name\"}, inplace=True)\n",
    "    per_class_df.rename(\n",
    "        columns={\n",
    "            \"class_name\": \"Class Name\",\n",
    "            \"accuracy\": \"Accuracy\",\n",
    "            \"precision\": \"Precision\",\n",
    "            \"recall\": \"Recall\",\n",
    "            \"f1\": \"F1 Score\",\n",
    "            \"support\": \"Support\",\n",
    "            \"misclassified\": \"Misclassified Count\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    per_class_df.to_csv(os.path.join(SAVE_DIR, \"id_per_class_metrics.csv\"), index=False)\n",
    "\n",
    "    miscls_df = pd.DataFrame(miscls)\n",
    "    miscls_df.rename(\n",
    "        columns={\n",
    "            \"img_path\": \"Image Path\",\n",
    "            \"true\": \"True Label\",\n",
    "            \"pred\": \"Predicted Label\",\n",
    "            \"score\": \"Prediction Confidence\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    miscls_df.to_csv(os.path.join(SAVE_DIR, \"misclassified_samples.csv\"), index=False)\n",
    "\n",
    "    n = min(9, len(miscls_df))\n",
    "    rows = (n + 2) // 3\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(n):\n",
    "        row = miscls_df.iloc[i]\n",
    "        img = Image.open(row[\"Image Path\"])\n",
    "        ax = plt.subplot(rows, 3, i + 1)\n",
    "        ax.imshow(img)\n",
    "        ax.axis(\"off\")\n",
    "        title_str = (\n",
    "            f\"Pred: {row['Predicted Label']}\\n\"\n",
    "            f\"True: {row['True Label']}\\n\"\n",
    "            f\"Conf: {row['Prediction Confidence']:.2f}\"\n",
    "        )\n",
    "        ax.set_title(title_str, fontsize=10, pad=5)\n",
    "\n",
    "    plt.suptitle(\"Misclassified Samples\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"misclassified_samples.png\"), dpi=600)\n",
    "    plt.close()\n",
    "\n",
    "    df_false_rejects = pd.DataFrame(report[\"false_rejects\"])\n",
    "    df_false_rejects.rename(\n",
    "        columns={\n",
    "            \"path1\": \"Image 1: Path\",\n",
    "            \"path2\": \"Image 2: Path\",\n",
    "            \"score\": \"Similarity Score\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    df_false_rejects.to_csv(os.path.join(SAVE_DIR, \"false_rejects.csv\"), index=False)\n",
    "\n",
    "    n = min(6, len(df_false_rejects))\n",
    "    rows = (n + 2) // 3\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(n):\n",
    "        row = df_false_rejects.iloc[i]\n",
    "        img1 = Image.open(row[\"Image 1: Path\"])\n",
    "        img2 = Image.open(row[\"Image 2: Path\"])\n",
    "        target_width = min(img1.width, img2.width)\n",
    "        target_height = min(img1.height, img2.height)\n",
    "        new_size = (target_width, target_height)\n",
    "\n",
    "        img1_resized = img1.resize(new_size)\n",
    "        img2_resized = img2.resize(new_size)\n",
    "\n",
    "        combined = Image.new(\"RGB\", (new_size[0] * 2, new_size[1]))\n",
    "        combined.paste(img1_resized, (0, 0))\n",
    "        combined.paste(img2_resized, (new_size[0], 0))\n",
    "        ax = plt.subplot(rows, 3, i + 1)\n",
    "        ax.imshow(combined)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        title_str = f\"Genuine -> Rejected\\nScore: {row['Similarity Score']:.2f}\"\n",
    "        ax.set_title(title_str, fontsize=10)\n",
    "    plt.suptitle(\"Unverified Genuine Pairs (False Rejects)\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"false_rejects.png\"), dpi=600)\n",
    "    plt.close()\n",
    "\n",
    "    df_false_accepts = pd.DataFrame(report[\"false_accepts\"])\n",
    "    df_false_accepts.rename(\n",
    "        columns={\n",
    "            \"path1\": \"Image 1: Path\",\n",
    "            \"path2\": \"Image 2: Path\",\n",
    "            \"score\": \"Similarity Score\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    df_false_accepts.to_csv(os.path.join(SAVE_DIR, \"false_accepts.csv\"), index=False)\n",
    "\n",
    "    n = min(6, len(df_false_accepts))\n",
    "    rows = (n + 2) // 3\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(n):\n",
    "        row = df_false_accepts.iloc[i]\n",
    "        img1 = Image.open(row[\"Image 1: Path\"])\n",
    "        img2 = Image.open(row[\"Image 2: Path\"])\n",
    "        target_width = min(img1.width, img2.width)\n",
    "        target_height = min(img1.height, img2.height)\n",
    "        new_size = (target_width, target_height)\n",
    "\n",
    "        img1_resized = img1.resize(new_size)\n",
    "        img2_resized = img2.resize(new_size)\n",
    "\n",
    "        combined = Image.new(\"RGB\", (new_size[0] * 2, new_size[1]))\n",
    "        combined.paste(img1_resized, (0, 0))\n",
    "        combined.paste(img2_resized, (new_size[0], 0))\n",
    "        ax = plt.subplot(rows, 3, i + 1)\n",
    "        ax.imshow(combined)\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "        title_str = f\"Impostor -> Accepted\\nScore: {row['Similarity Score']:.2f}\"\n",
    "        ax.set_title(title_str, fontsize=10)\n",
    "    plt.suptitle(\"Verified Impostor Pairs (False Accepts)\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(SAVE_DIR, \"false_accepts.png\"), dpi=600)\n",
    "    plt.close()\n",
    "\n",
    "    rename_dict = {\n",
    "        \"feat_dim\": \"feature_dimensions\",\n",
    "        \"dropout\": \"dropout_rate\",\n",
    "        \"s\": \"arcface_scaling_factor\",\n",
    "        \"m\": \"arcface_angular_margin_penalty\",\n",
    "        \"lambda_c\": \"lambda_center\",\n",
    "        \"lr\": \"learning_rate\",\n",
    "        \"wd\": \"weight_decay\",\n",
    "        \"lr_center\": \"learning_rate_center\",\n",
    "        \"wd_center\": \"weight_decay_center\",\n",
    "        \"momentum_center\": \"momentum_center\",\n",
    "        \"T_0\": \"scheduler_T0\",\n",
    "        \"eta_min\": \"scheduler_eta_min\",\n",
    "        \"step_size_center\": \"scheduler_center_step_size\",\n",
    "        \"gamma_center\": \"scheduler_center_gamma\",\n",
    "    }\n",
    "    metrics = {}\n",
    "    best_params = {rename_dict.get(k, k): v for k, v in best_params.items()}\n",
    "    metrics[\"best_params\"] = best_params\n",
    "    metrics[\"decision_thresholds\"] = {\n",
    "        \"osr_decision_threshold\": osr_threshold,\n",
    "        \"verification_decision_threshold\": verif_threshold,\n",
    "    }\n",
    "    metrics[\"identification\"] = {\n",
    "        key: value\n",
    "        for key, value in metrics_osr_id.items()\n",
    "        if key.startswith(\"identification_\")\n",
    "        and key != \"identification_per_class_metrics\"\n",
    "    }\n",
    "    metrics[\"open_set\"] = {\n",
    "        key: value\n",
    "        for key, value in metrics_osr_id.items()\n",
    "        if key.startswith(\"open_set_\")\n",
    "    }\n",
    "    metrics[\"verification\"] = metrics_verif\n",
    "    with open(os.path.join(SAVE_DIR, \"final_results.json\"), \"w\") as f:\n",
    "        json.dump(metrics, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26787b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_dataset = filter_dataset(IMAGE_DIR)\n",
    "known_classes, unknown_classes = prepare_dataset_splits(IMAGE_DIR)\n",
    "unknown_val, unknown_test = prepare_unknown_splits(filtered_dataset, unknown_classes)\n",
    "num_known = len(known_classes)\n",
    "\n",
    "dist_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Label\": list(filtered_dataset.keys()),\n",
    "        \"Support\": [len(images) for images in filtered_dataset.values()],\n",
    "    }\n",
    ").set_index(\"Label\")\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "ax = plt.gca()\n",
    "dist_df.plot(kind=\"bar\", legend=False, ax=ax)\n",
    "plt.ylabel(\"Number of Images\")\n",
    "plt.xlabel(\"Class Label\")\n",
    "ax.set_xticklabels(dist_df.index, fontsize=10)\n",
    "plt.title(\"Dataset Distribution per Class\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(SAVE_DIR, \"dataset_distribution.png\"), dpi=600)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03894e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model(\n",
    "    filtered_dataset,\n",
    "    known_classes,\n",
    "    unknown_val,\n",
    "    unknown_test,\n",
    "    num_known,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    best_params=cross_validate(\n",
    "        filtered_dataset,\n",
    "        known_classes,\n",
    "        unknown_val,\n",
    "        unknown_test,\n",
    "        num_known,\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a12c1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(\"weights.zip\", \"w\", zipfile.ZIP_DEFLATED) as zip_ref:\n",
    "    for file in os.listdir(SAVE_DIR):\n",
    "        if file.endswith((\".csv\", \".json\", \".pkl\", \".png\", \".pt\")):\n",
    "            file_path = os.path.join(SAVE_DIR, file)\n",
    "            zip_ref.write(file_path, os.path.relpath(file_path, SAVE_DIR))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
